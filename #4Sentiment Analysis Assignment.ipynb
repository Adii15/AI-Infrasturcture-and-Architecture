{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ddd9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the \"fer2013\" dataset\n",
    "data = pd.read_csv('D:/face/train.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "images = []\n",
    "for pixels in data['pixels']:\n",
    "    img = np.array([int(pixel) for pixel in pixels.split(' ')], dtype=np.uint8).reshape((48, 48))\n",
    "    images.append(img)\n",
    "\n",
    "X = np.array(images)\n",
    "y = data['emotion']\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X = X / 255.0\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0add6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "718/718 [==============================] - 21s 29ms/step - loss: 1.7028 - accuracy: 0.3155 - val_loss: 1.5148 - val_accuracy: 0.4248\n",
      "Epoch 2/20\n",
      "718/718 [==============================] - 24s 33ms/step - loss: 1.4492 - accuracy: 0.4437 - val_loss: 1.3880 - val_accuracy: 0.4688\n",
      "Epoch 3/20\n",
      "718/718 [==============================] - 28s 39ms/step - loss: 1.3218 - accuracy: 0.4955 - val_loss: 1.2960 - val_accuracy: 0.5094\n",
      "Epoch 4/20\n",
      "718/718 [==============================] - 27s 37ms/step - loss: 1.2421 - accuracy: 0.5268 - val_loss: 1.2807 - val_accuracy: 0.5148\n",
      "Epoch 5/20\n",
      "718/718 [==============================] - 26s 36ms/step - loss: 1.1819 - accuracy: 0.5536 - val_loss: 1.2575 - val_accuracy: 0.5294\n",
      "Epoch 6/20\n",
      "718/718 [==============================] - 26s 36ms/step - loss: 1.1237 - accuracy: 0.5777 - val_loss: 1.2352 - val_accuracy: 0.5340\n",
      "Epoch 7/20\n",
      "718/718 [==============================] - 26s 36ms/step - loss: 1.0705 - accuracy: 0.5993 - val_loss: 1.2416 - val_accuracy: 0.5333\n",
      "Epoch 8/20\n",
      "718/718 [==============================] - 26s 36ms/step - loss: 1.0196 - accuracy: 0.6195 - val_loss: 1.2232 - val_accuracy: 0.5515\n",
      "Epoch 9/20\n",
      "718/718 [==============================] - 26s 36ms/step - loss: 0.9708 - accuracy: 0.6411 - val_loss: 1.2390 - val_accuracy: 0.5401\n",
      "Epoch 10/20\n",
      "718/718 [==============================] - 26s 36ms/step - loss: 0.9200 - accuracy: 0.6581 - val_loss: 1.2337 - val_accuracy: 0.5482\n",
      "Epoch 11/20\n",
      "718/718 [==============================] - 26s 36ms/step - loss: 0.8711 - accuracy: 0.6776 - val_loss: 1.2924 - val_accuracy: 0.5509\n",
      "Epoch 12/20\n",
      "718/718 [==============================] - 27s 37ms/step - loss: 0.8236 - accuracy: 0.6987 - val_loss: 1.3007 - val_accuracy: 0.5509\n",
      "Epoch 13/20\n",
      "718/718 [==============================] - 27s 38ms/step - loss: 0.7762 - accuracy: 0.7155 - val_loss: 1.3564 - val_accuracy: 0.5343\n",
      "Epoch 14/20\n",
      "718/718 [==============================] - 27s 37ms/step - loss: 0.7250 - accuracy: 0.7351 - val_loss: 1.4019 - val_accuracy: 0.5390\n",
      "Epoch 15/20\n",
      "718/718 [==============================] - 27s 38ms/step - loss: 0.6763 - accuracy: 0.7507 - val_loss: 1.4409 - val_accuracy: 0.5362\n",
      "Epoch 16/20\n",
      "718/718 [==============================] - 27s 38ms/step - loss: 0.6296 - accuracy: 0.7701 - val_loss: 1.5346 - val_accuracy: 0.5392\n",
      "Epoch 17/20\n",
      "718/718 [==============================] - 31s 43ms/step - loss: 0.5834 - accuracy: 0.7870 - val_loss: 1.5561 - val_accuracy: 0.5259\n",
      "Epoch 18/20\n",
      "718/718 [==============================] - 31s 44ms/step - loss: 0.5362 - accuracy: 0.8068 - val_loss: 1.6687 - val_accuracy: 0.5340\n",
      "Epoch 19/20\n",
      "718/718 [==============================] - 34s 47ms/step - loss: 0.4919 - accuracy: 0.8246 - val_loss: 1.7327 - val_accuracy: 0.5282\n",
      "Epoch 20/20\n",
      "718/718 [==============================] - 35s 49ms/step - loss: 0.4590 - accuracy: 0.8335 - val_loss: 1.9187 - val_accuracy: 0.5251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1db25a581f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(7, activation='softmax')  # 7 classes for 7 emotions (anger, disgust, fear, happy, sad, surprise, neutral)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape the data to fit the model input\n",
    "X_train_reshaped = X_train.reshape(-1, 48, 48, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fd8d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 2s 11ms/step - loss: 1.9187 - accuracy: 0.5251\n",
      "Test Accuracy: 0.5250783562660217\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88522120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to an HDF5 file\n",
    "model.save('emotion_detection_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbf32028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 60ms/step\n",
      "Emotion: happy\n",
      "Recommended songs:\n",
      "hiphop\n",
      "hiphop\n",
      "hiphop\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Emotion: fear\n",
      "Recommended songs:\n",
      "classical\n",
      "classical\n",
      "classical\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Emotion: happy\n",
      "Recommended songs:\n",
      "hiphop\n",
      "hiphop\n",
      "hiphop\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Emotion: neutral\n",
      "Recommended songs:\n",
      "blues\n",
      "blues\n",
      "blues\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Emotion: sad\n",
      "Recommended songs:\n",
      "blues\n",
      "blues\n",
      "blues\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import time\n",
    "import pygame\n",
    "\n",
    "video_url = 'http://192.168.2.117:4747/video'\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "# Check if the video stream was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream.\")\n",
    "    exit()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained emotion detection model\n",
    "emotion_model = tf.keras.models.load_model('C:/Users/adity/Downloads/emotion_detection_model1.h5')\n",
    "\n",
    "# Define the emotion labels (same as in the previous code)\n",
    "emotion_labels = ['anger', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load the music features from the CSV file\n",
    "music_features_data = pd.read_csv('D:/face/Data/features_3_sec.csv')\n",
    "\n",
    "# Extract the relevant features for music recommendation (same as the emotion detection features)\n",
    "X_music_features = music_features_data.iloc[:, 2:-1].values\n",
    "\n",
    "# Normalize the music features\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_music_features = X_music_features / 255.0\n",
    "\n",
    "# Create a list to store recommended songs for each emotion\n",
    "song_recommendations = {\n",
    "    'anger': ['rock', 'rock', 'rock'],\n",
    "    'disgust': ['hiphop', 'hiphop', 'hiphop'],\n",
    "    'fear': ['classical', 'classical', 'classical'],\n",
    "    'happy': ['hiphop', 'hiphop', 'hiphop'],\n",
    "    'sad': ['blues', 'blues', 'blues'],\n",
    "    'surprise': ['hiphop', 'hiphop', 'hiphop'],\n",
    "    'neutral': ['blues', 'blues', 'blues']\n",
    "}\n",
    "\n",
    "# Define the path where songs are stored\n",
    "songs_path = 'D:/face/Data/genres_original/'\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if there are no more frames\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Calculate the time elapsed since the last prediction\n",
    "    current_time = time.time()\n",
    "    time_elapsed = current_time - prev_time\n",
    "\n",
    "    # Perform emotion prediction and recommend a song after every 5 seconds\n",
    "    if time_elapsed >= 5.0:\n",
    "        prev_time = current_time\n",
    "\n",
    "        # Preprocess the frame (convert to grayscale, resize to 48x48, normalize)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        resized_frame = cv2.resize(gray_frame, (48, 48))\n",
    "        normalized_frame = resized_frame / 255.0\n",
    "        input_frame = normalized_frame.reshape(-1, 48, 48, 1)\n",
    "\n",
    "        # Perform emotion detection on the frame\n",
    "        predicted_emotions = emotion_model.predict(input_frame)\n",
    "        predicted_emotion_class = predicted_emotions.argmax(axis=1)[0]\n",
    "        predicted_emotion = emotion_labels[predicted_emotion_class]\n",
    "\n",
    "        # Get song recommendations for the predicted emotion\n",
    "        recommended_songs = song_recommendations[predicted_emotion]\n",
    "\n",
    "        # Print the emotion label and recommended songs\n",
    "        print(f\"Emotion: {predicted_emotion}\")\n",
    "        print(\"Recommended songs:\")\n",
    "        for song in recommended_songs:\n",
    "            print(song)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Play the first recommended song from the provided path\n",
    "        if recommended_songs:\n",
    "            song_path = songs_path + recommended_songs[0] +'/'+ recommended_songs[0] + '.00000.wav'\n",
    "            pygame.mixer.music.load(song_path)\n",
    "            pygame.mixer.music.play()\n",
    "\n",
    "    # Show the frame with the emotion label\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video stream and close the OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb81c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
